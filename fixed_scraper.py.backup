import time
import csv
import re
import os
from random import randint
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException


def extract_detail_panel(driver, listing_data=None):
    """Extract data from the right side detail panel after clicking"""
    business_data = listing_data.copy() if listing_data else {}
    if 'name' not in business_data: business_data['name'] = "N/A"

    try:
        # Define selectors
        name_selectors = [
            'h2.qrShPb',
            'h1.DUwDvf',
            'div.SPZz6b h2',
            'div.SPZz6b h1',
            'div.x0H67.r9fE8',
            'div.v93No.H7V2N.fEByN',
            '[role="heading"]',
            '.rG09U',
            '.H07f0c',
            'div.PZPZ1c h2',
            'div.PZPZ1c h1'
        ]

        # Wait for any name selector to appear
        wait = WebDriverWait(driver, 4) # Further reduced wait time
        nameFound = business_data.get('name') != "N/A"

        if not nameFound:
            for selector in name_selectors:
                try:
                    name_elem = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, selector)))
                    name = name_elem.text.strip()
                    if name:
                        business_data['name'] = name
                        nameFound = True
                        break
                except:
                    continue

        if not nameFound:
            # Fallback: check all headings immediately
            try:
                headings = driver.find_elements(By.CSS_SELECTOR, 'h1, h2, [role="heading"]')
                for h in headings:
                    if h.is_displayed():
                        t = h.text.strip()
                        if t and len(t) > 2:
                            business_data['name'] = t
                            nameFound = True
                            break
            except:
                pass

        if not nameFound:
            business_data['name'] = "N/A"
            print("   DEBUG: Name not found in panel")

        # Rating - More specific selectors to target the main business rating
        rating_selectors = [
            'div[role="main"] span.ceNzKf[aria-hidden="true"]',
            'div.DUwDvf span.ceNzKf[aria-hidden="true"]',  # Near main heading
            'div.qrShPb span.ceNzKf[aria-hidden="true"]',  # Near main heading
            'div.F7nice span[aria-hidden="true"]:first-child',  # First rating in F7nice div
            'div[role="main"] div.F7nice span[aria-hidden="true"]',
            'span.Aq14fc',
            'span.gsrt.By079',
            'div.PZPZ1c span[aria-hidden="true"]',
            'span.ceNzKf[aria-hidden="true"]'  # Fallback to original
        ]

        for selector in rating_selectors:
            try:
                rating_elem = driver.find_element(By.CSS_SELECTOR, selector)
                rating = rating_elem.text.strip()
                # Validate that this is a legitimate rating (between 0.0 and 5.0)
                if rating:
                    try:
                        rating_float = float(rating)
                        if 0.0 <= rating_float <= 5.0:  # Ratings should be between 0-5
                            business_data['rating'] = rating
                            print(f"DEBUG - Found rating: {rating} with selector: {selector}")
                            break
                        else:
                            print(f"DEBUG - Skipping invalid rating: {rating} (out of range)")
                    except ValueError:
                        print(f"DEBUG - Skipping non-numeric rating: {rating}")
            except:
                # Just silently continue to next selector if element not found
                continue

        if 'rating' not in business_data:
            business_data['rating'] = "N/A"

        # Total Reviews - Fixed to properly extract review count
        total_reviews_found = False

        try:
            # Look specifically for the review count that's associated with the rating
            # This is typically in the same container as the rating
            review_selectors = [
                'div.F7nice span[aria-label*="review"]',  # Reviews element with label
                'div.F7nice span.RDApEe.YrbPuc',  # Reviews in the rating section
                'div.F7nice span:last-of-type:not([aria-hidden="true"])',  # Last visible span in rating area
                'div.DUwDvf + div span.RDApEe.YrbPuc',  # Reviews element right after the title
                'span.RDApEe.YrbPuc'  # Original selector - this targets the specific class you mentioned
            ]

            for selector in review_selectors:
                try:
                    reviews_elems = driver.find_elements(By.CSS_SELECTOR, selector)

                    for reviews_elem in reviews_elems:
                        # Get the parent to check if it's in the rating area
                        try:
                            parent = reviews_elem.find_element(By.XPATH, './..')
                            parent_class = parent.get_attribute('class') or ''
                        except:
                            parent_class = ''

                        # Check if this element is in the rating area
                        is_near_rating = 'F7nice' in parent_class or 'rating' in parent_class.lower()

                        # Get attributes
                        aria_label = reviews_elem.get_attribute('aria-label')
                        elem_text = reviews_elem.text.strip()

                        # Process regardless of position for the specific RDApEe.YrbPuc selector
                        is_target_selector = 'RDApEe.YrbPuc' in selector

                        # Only process if this element is in the rating area OR it's our target selector
                        if is_near_rating or is_target_selector:
                            # Extract from aria-label first (higher priority)
                            if aria_label:
                                # Look for patterns like "416 user reviews" - this handles the exact format you mentioned
                                aria_match = re.search(r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?[KkMm]?)\s+(?:user reviews|reviews?|ratings?)', aria_label, re.IGNORECASE)
                                if aria_match:
                                    business_data['total_reviews'] = aria_match.group(1).replace(',', '')
                                    print(f"DEBUG - Found reviews from aria-label: {business_data['total_reviews']}")
                                    total_reviews_found = True
                                    break

                            # If not found in aria-label, extract from text content (the (416) format)
                            if elem_text:
                                # Look for parenthesized numbers (e.g., "(416)") - this handles your specific case
                                paren_match = re.search(r'\(([\d,]+[KkMm]?)\)', elem_text)
                                if paren_match:
                                    business_data['total_reviews'] = paren_match.group(1).replace(',', '')
                                    print(f"DEBUG - Found reviews from text (parentheses): {business_data['total_reviews']}")
                                    total_reviews_found = True
                                    break

                                # Look for numbers in the text as a fallback
                                num_match = re.search(r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?[KkMm]?)', elem_text)
                                if num_match:
                                    business_data['total_reviews'] = num_match.group(1).replace(',', '')
                                    print(f"DEBUG - Found reviews from text (fallback): {business_data['total_reviews']}")
                                    total_reviews_found = True
                                    break

                except Exception as e:
                    print(f"DEBUG - Selector {selector} failed: {e}")
                    continue

                if total_reviews_found:
                    break

        except Exception as e:
            print(f"DEBUG - Error in reviews extraction: {e}")

        if not total_reviews_found:
            business_data['total_reviews'] = "N/A"
            print("DEBUG - No reviews found, setting to N/A")
        else:
            print(f"DEBUG - Final reviews value: {business_data['total_reviews']}")

        # Category/Type
        category_selectors = [
            'button.DkEaL',
            'span.YhemCb',
            'div.LBgpqf button',
            'div.PZPZ1c span:nth-of-type(1)'
        ]

        for selector in category_selectors:
            try:
                category = driver.find_element(By.CSS_SELECTOR, selector).text.strip()
                if category and len(category) < 50:
                    business_data['category'] = category
                    break
            except:
                continue

        if 'category' not in business_data:
            business_data['category'] = "N/A"

        # Address
        address_selectors = [
            'span.LrzXr',
            'button[data-item-id="address"]',
            'button[data-tooltip*="Address"]',
            'div.rogA2c[data-item-id="address"]',
            'div[data-item-id="address"]',
            'span.fMghS'
        ]

        for selector in address_selectors:
            try:
                addr_elem = driver.find_element(By.CSS_SELECTOR, selector)
                address = addr_elem.get_attribute('aria-label')
                if not address:
                    address = addr_elem.text.strip()

                if address:
                    address = address.replace('Address: ', '').replace('Copy address', '').strip()
                    if address:
                        business_data['address'] = address
                        break
            except:
                continue

        if 'address' not in business_data:
            business_data['address'] = "N/A"

        # Phone Number
        phone_selectors = [
            'button[data-item-id*="phone"]',
            'button[aria-label*="Phone"]',
            'a[data-item-id*="phone"]',
            'a[data-dtype="d3ph"]',
            'span.LrzXr.zdqRlf.kno-fv a',
            'span.w8qArf.FoJoyf a.fl',
            'span.LrzXr',
            'span[data-dtype="d3ph"]'
        ]

        for selector in phone_selectors:
            try:
                phone_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for phone_elem in phone_elements:
                    phone = phone_elem.get_attribute('aria-label')
                    if not phone:
                        phone = phone_elem.text.strip()

                    if phone:
                        phone = phone.replace('Phone: ', '').replace('Copy phone number', '').strip()
                        phone = phone.replace('Call', '').replace('phone number', '').strip()

                        if phone and ('+' in phone or sum(c.isdigit() for c in phone) >= 7):
                            phone_match = re.search(r'(\+?\d[\d\s\-\(\)]{7,})', phone)
                            if phone_match:
                                business_data['phone'] = phone_match.group(1).strip()
                            else:
                                business_data['phone'] = phone
                            break

                if 'phone' in business_data:
                    break
            except:
                continue

        if 'phone' not in business_data:
            business_data['phone'] = "N/A"

        # Website
        website_selectors = [
            'a.n1obkb.mI8Pwc',
            'a[data-item-id="authority"]',
            'a[aria-label*="Website"]',
            'button[data-item-id="authority"]',
            'a.ab_button[href*="http"]'
        ]

        for selector in website_selectors:
            try:
                website_elem = driver.find_element(By.CSS_SELECTOR, selector)
                website = website_elem.get_attribute('href')
                if not website:
                    website = website_elem.text.strip()

                if website and 'http' in website and 'google' not in website.lower():
                    business_data['website'] = website
                    break
            except:
                continue

        if 'website' not in business_data:
            business_data['website'] = "N/A"

        # Price Range
        price_selectors = [
            'span[aria-label*="Price"]',
            'span.mgr77e',
            'span.YhemCb'
        ]

        for selector in price_selectors:
            try:
                price = driver.find_element(By.CSS_SELECTOR, selector).text.strip()
                if price and '$' in price:
                    business_data['price_range'] = price
                    break
            except:
                continue

        if 'price_range' not in business_data:
            business_data['price_range'] = "N/A"

        # Hours Status
        hours_selectors = [
            'div.OqCZI',
            'span[aria-label*="Hours"]',
            'div.MkXq9e',
            'div.J77u9c'
        ]

        for selector in hours_selectors:
            try:
                hours = driver.find_element(By.CSS_SELECTOR, selector).text.strip()
                if hours:
                    business_data['hours_status'] = hours
                    break
            except:
                continue

        if 'hours_status' not in business_data:
            business_data['hours_status'] = "N/A"

        business_data['google_maps_url'] = driver.current_url

        return business_data

    except Exception as e:
        print(f"[ERROR] Error extracting detail panel: {e}")
        return None


def scrape_current_page(driver, all_businesses, csv_filepath, fieldnames, termination_flag=None):
    """Scrape listings from the current search results page"""
    wait = WebDriverWait(driver, 8)  # Reduced from 15 to 8 seconds

    # Check termination before starting
    if termination_flag and termination_flag():
        return all_businesses

    # Handle consent if appears
    try:
        # Check termination before handling consent
        if termination_flag and termination_flag():
            return all_businesses
        consent = driver.find_element(By.XPATH, "//button[contains(., 'Accept all') or contains(., 'Reject all')]")
        consent.click()
        time.sleep(1)  # Reduced from 2 to 1 second
    except:
        pass

    # Find clickable listings (Order matched to test.py)
    clickable_selectors = [
        'a.vwVdIc',
        'div.VkpGBb a',
        'a[jsname]',
        'div[role="article"] a',
        'div.g a',
        'a.sVXRqc',
        'a[data-cid]',
        'div.tF2Cxc a',
    ]

    listings = []
    successful_selector = None

    for selector in clickable_selectors:
        # Check termination before trying each selector
        if termination_flag and termination_flag():
            return all_businesses
        try:
            listings = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector)))
            if len(listings) > 0:
                successful_selector = selector
                print(f"Found {len(listings)} listings using: {selector}")
                break
        except:
            continue

    if not listings:
        print("[WARNING] DEBUG: No clickable listings found on this page")
        return all_businesses
    else:
        print(f"DEBUG: Found {len(listings)} listings using {successful_selector}")

    total_to_scrape = len(listings)
    print(f"Will scrape ALL {total_to_scrape} listings from current page")

    for i in range(total_to_scrape):
        # Check termination at the beginning of each listing iteration
        if termination_flag and termination_flag():
            print(f"[TERMINATION] Terminating during scraping of listing {i+1}")
            return all_businesses

        try:
            print(f"\n[{len(all_businesses)+1}] Processing listing {i+1}...")

            # Re-find listings to avoid stale elements
            current_listings = driver.find_elements(By.CSS_SELECTOR, successful_selector)
            print(f"DEBUG: Found {len(current_listings)} listings on page, attempting index {i}")
            if i >= len(current_listings):
                print(f"[WARNING] Ran out of listings at index {i}, only {len(current_listings)} available")
                break

            listing = current_listings[i]
            # Print text content of the listing to verify it's different
            listing_preview = listing.text[:30] if listing.text else "NO TEXT"
            print(f"DEBUG: Selected listing {i+1} for processing, content preview: '{listing_preview}...'")

            # CLICK AND EXTRACT
            # 1. Capture basic info from listing first (fallback)
            listing_data = {'name': 'N/A', 'address': 'N/A', 'rating': 'N/A', 'total_reviews': 'N/A'}
            try:
                # Try to find name/address in the result card before clicking
                card_text = listing.text
                if card_text:
                    lines = [line.strip() for line in card_text.split('\n') if line.strip()]
                    if lines:
                        listing_data['name'] = lines[0] # Often the first line is the name
            except:
                pass

            # Scroll into view
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", listing)
            time.sleep(randint(0, 1))  # Reduced from randint(1, 2) to randint(0, 1)

            # Check termination before clicking
            if termination_flag and termination_flag():
                print(f"[TERMINATION] Terminating before clicking listing {i+1}")
                return all_businesses

            # Get listing information before clicking
            listing_text = listing.text[:50] if listing.text else "NO TEXT"
            print(f"DEBUG: About to click listing {i+1}, text preview: '{listing_text}...'")

            # Click
            try:
                # Try clicking the parent or the link
                listing.click()
            except:
                try:
                    driver.execute_script("arguments[0].click();", listing)
                except Exception as click_error:
                    print(f"DEBUG: Click failed for listing {i+1}, error: {click_error}")
                    pass

            # Wait for content or panel to load with dynamic checking
            print(f"DEBUG: Waiting for detail page to load after click {i+1}")
            wait = WebDriverWait(driver, 8)  # Reduced timeout
            try:
                # Wait for the main content to be present instead of fixed time
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.DUwDvf, h2.qrShPb, [role='heading']")))
            except TimeoutException:
                # If main elements don't load, wait a bit more but check termination
                for _ in range(3):
                    if termination_flag and termination_flag():
                        print(f"[TERMINATION] Terminating during wait for panel load")
                        return all_businesses
                    time.sleep(0.5)

            # Extract detailed data
            current_page_url = driver.current_url
            print(f"DEBUG: About to extract detail panel for listing {i+1}, current URL: {current_page_url[-50:]}")
            business_data = extract_detail_panel(driver, listing_data)

            if business_data and business_data.get('name') != "N/A":
                all_businesses.append(business_data)

                # Save to CSV immediately and flush to disk
                with open(csv_filepath, 'a', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writerow(business_data)
                    csvfile.flush()

                # Display comprehensive information
                name = business_data.get('name', 'N/A')
                rating = business_data.get('rating', 'N/A')
                reviews = business_data.get('total_reviews', 'N/A')
                category = business_data.get('category', 'N/A')
                address = business_data.get('address', 'N/A')

                print(f"[INFO] Scraped: {name}")
                print(f"   Rating: {rating} - Reviews: {reviews}")
                print(f"   Category: {category}")
                print(f"   Address: {address}")
                print("-" * 80)

                # Verify data integrity
                print(f"DEBUG: Full business data keys: {list(business_data.keys())}")
                print(f"DEBUG: Rating value: '{rating}', Reviews value: '{reviews}'")
            else:
                print("[WARNING] Could not extract data for this item")
                print(f"DEBUG: business_data was: {business_data}")

            # Check termination before going back
            if termination_flag and termination_flag():
                print(f"[TERMINATION] Terminating before going back from listing {i+1}")
                return all_businesses

            # Store the URL before going back to check if navigation was successful
            initial_url = driver.current_url
            print(f"DEBUG: Current URL before going back: {initial_url[:100]}...")

            # Go back to search results
            driver.back()

            # Wait briefly for the back action to process
            time.sleep(1)

            # Check if we actually navigated back
            after_back_url = driver.current_url
            print(f"DEBUG: URL after going back: {after_back_url[:100]}...")

            if initial_url == after_back_url:
                print("DEBUG: Warning - URL unchanged after back() command, page might not have navigated back")
            else:
                print("DEBUG: Successfully navigated back to search results")

            # Wait for the search results page to load completely
            # This is crucial to ensure we get fresh listings for the next iteration
            try:
                # Wait for the search results container to be present again
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, successful_selector)))
            except:
                # If we can't find the selector, verify we're back on search page
                try:
                    # Look for typical search results elements
                    search_results_container = driver.find_element(By.CSS_SELECTOR, 'div#search, div#ires, div.g, div.tF2Cxc')
                    print("DEBUG: Confirmed back on search results page")
                except:
                    print("DEBUG: Might not be back on search results page")
                    time.sleep(1)  # Minimal wait if still not sure

            # Check termination after going back
            if termination_flag and termination_flag():
                print(f"[TERMINATION] Terminating after going back from listing {i+1}")
                return all_businesses

            time.sleep(randint(0, 1))  # Reduced wait time

        except Exception as e:
            print(f"[ERROR] Error on listing {i+1}: {e}")
            try:
                # Check termination before going back on error
                if termination_flag and termination_flag():
                    print(f"[TERMINATION] Terminating during error recovery for listing {i+1}")
                    return all_businesses
                driver.back()
                time.sleep(1)  # Reduced from 2 seconds
            except:
                pass
            continue

    return all_businesses


def click_next_page_with_termination(driver, termination_flag):
    """Click next page button with termination check - IMPROVED VERSION from test.py"""

    # Check termination before starting
    if termination_flag and termination_flag():
        return False

    # Scroll to bottom first
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1)  # Reduced from 2 to 1 second

    # Check termination after scrolling
    if termination_flag and termination_flag():
        return False

    # Try multiple next button selectors
    next_selectors = [
        'a#pnnext',  # Standard Google next button
        'a[aria-label*="Next"]',
        'a[aria-label*="next"]',
        'td.b a',  # Pagination table cell
        'span.SJajHc.NVbCr a',  # Google pagination
        'a.nBDE1b.G5eFlf',  # Another pagination selector
        '#pnnext',
        'span[style*="background:url"] a',  # Next arrow image
        'table#nav td:last-child a',  # Last pagination cell
    ]

    for selector in next_selectors:
        # Check termination before trying each selector
        if termination_flag and termination_flag():
            return False

        try:
            print(f"üîç Trying selector: {selector}")

            # Wait for element with shorter timeout
            wait = WebDriverWait(driver, 5)
            next_button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))

            # Check termination after finding element
            if termination_flag and termination_flag():
                return False

            # Check if button is enabled (not greyed out)
            aria_label = next_button.get_attribute('aria-label')
            if aria_label and 'next' in aria_label.lower():
                print(f"‚úì Found next button: {aria_label}")

            # Scroll to button
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)

            # Check termination after scrolling to button
            if termination_flag and termination_flag():
                return False

            time.sleep(0.5)  # Reduced from 1 to 0.5 seconds

            # Try clicking
            try:
                next_button.click()
            except:
                driver.execute_script("arguments[0].click();", next_button)

            print("‚úÖ Next button clicked successfully!")
            time.sleep(randint(2, 4))  # Reduced wait for page load

            # Check termination after clicking
            if termination_flag and termination_flag():
                return False

            return True

        except Exception as e:
            continue

    print("‚ùå Could not find next button")
    return False


def scrape_google_search(search_query, location="", csv_filepath="", fieldnames=None, termination_flag=None, job_id=None, thread_drivers=None):
    """Main scraping with UNLIMITED pagination - Scrapes ALL available results from test.py"""

    import urllib.parse
    driver = None
    try:
        # Check termination flag before starting
        if termination_flag and termination_flag():
            print(f"[TERMINATION] Job terminated before starting for query: {search_query}")
            return []

        # Chrome Options from test.py
        option = webdriver.ChromeOptions()
        # option.add_argument("--headless")
        option.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
        option.add_argument("--disable-blink-features=AutomationControlled")
        option.add_experimental_option("useAutomationExtension", False)
        option.add_argument('--disable-infobars')
        option.add_experimental_option("excludeSwitches", ["enable-automation"])
        option.add_argument("--start-maximized")
        option.add_argument("--disable-search-engine-choice-screen")
        option.add_argument("--disable-dev-shm-usage")
        option.add_argument("--no-sandbox")
        option.add_argument("--disable-gpu")
        option.add_argument("--disable-features=IsolateOrigins,site-per-process")
        # Suppress Errors and Noise
        option.add_argument("--log-level=3")
        option.add_argument("--silent")
        option.add_argument("--disable-logging")
        option.add_argument("--disable-background-networking")
        option.add_argument("--disable-sync")
        option.add_argument("--no-first-run")
        option.add_argument("--ignore-certificate-errors")
        option.add_argument("--ignore-ssl-errors")
        option.add_argument("--allow-running-insecure-content")

        print(f"DEBUG: Starting Chrome for query: {search_query}")
        try:
            driver = webdriver.Chrome(options=option)
            driver.set_page_load_timeout(8)  # Further reduced timeout to allow faster interruption
            # Hide automation flag from test.py
            driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            print(f"DEBUG: Browser opened successfully for query: {search_query}")

            # Register the driver with the job_id if provided and thread_drivers is not None
            if job_id and thread_drivers is not None:
                thread_drivers[job_id] = driver
        except Exception as e:
            print(f"[CRITICAL] Failed to initialize Chrome: {e}")
            return []

        if location:
            query = f"{search_query} in {location}"
        else:
            query = search_query

        search_url = f"https://www.google.com/search?q={urllib.parse.quote_plus(query)}&udm=1"

        print(f"\nüîç Searching: {query}")
        print(f"üåê URL: {search_url}")
        print(f"‚ôæÔ∏è  Mode: UNLIMITED - Will scrape ALL available pages!\n")

        # Check termination flag before navigating
        if termination_flag and termination_flag():
            print(f"[TERMINATION] Job terminated before navigation for query: {search_query}")
            return []

        driver.get(search_url)

        # Check termination flag after navigation
        if termination_flag and termination_flag():
            print(f"[TERMINATION] Job terminated after navigation for query: {search_query}")
            return []

        time.sleep(3)  # Reduced from 5 to 3 seconds

        # Handle consent - Improved from test.py
        print(f"DEBUG: Checking for consent/popups...")
        consent_button_locators = [
            (By.XPATH, "//button[contains(., 'Accept all')]"),
            (By.XPATH, "//button[contains(., 'Reject all')]"),
            (By.XPATH, "//button[contains(., 'I agree')]"),
            (By.ID, "L2AGLb"),
        ]

        for by, val in consent_button_locators:
            try:
                # Check termination flag before clicking consent
                if termination_flag and termination_flag():
                    print(f"[TERMINATION] Job terminated during consent for query: {search_query}")
                    return []

                btn = driver.find_element(by, val)
                if btn.is_displayed():
                    btn.click()
                    print(f"DEBUG: Clicked consent button: {val}")
                    time.sleep(1)  # Reduced from 2 to 1 second
                    break
            except:
                continue

        all_businesses = []
        page_num = 1
        max_pages = 100  # Safety limit to prevent infinite loops

        while page_num <= max_pages:
            # Check termination flag at the beginning of each page iteration
            if termination_flag and termination_flag():
                print(f"[TERMINATION] Job terminated during page {page_num} for query: {search_query}")
                return all_businesses

            print("\n" + "=" * 80)
            print(f"üìÑ SCRAPING PAGE {page_num}")
            print(f"üìä Current total: {len(all_businesses)}")
            print("=" * 80 + "\n")

            # Scrape current page with termination check
            previous_count = len(all_businesses)
            all_businesses = scrape_current_page(driver, all_businesses, csv_filepath, fieldnames, termination_flag)

            # Check if any new results were added
            new_results = len(all_businesses) - previous_count
            print(f"\n‚úÖ Scraped {new_results} new businesses from page {page_num}")

            # Check termination flag before moving to next page
            if termination_flag and termination_flag():
                print(f"[TERMINATION] Job terminated before moving to next page {page_num + 1} for query: {search_query}")
                return all_businesses

            # Try to go to next page
            print(f"\nüîÑ Attempting to navigate to page {page_num + 1}...")

            if click_next_page_with_termination(driver, termination_flag):
                page_num += 1
                print(f"‚úÖ Successfully moved to page {page_num}")

                # Check termination flag after moving to next page
                if termination_flag and termination_flag():
                    print(f"[TERMINATION] Job terminated after moving to page {page_num} for query: {search_query}")
                    return all_businesses
            else:
                print("‚ö†Ô∏è No more pages available")
                break

        print("\n" + "=" * 80)
        print(f"‚úÖ Scraping completed!")
        print(f"üìä Total businesses scraped: {len(all_businesses)}")
        print(f"üìÑ Total pages scraped: {page_num}")
        print("=" * 80)

        return all_businesses

    except Exception as e:
        print(f"[ERROR] ERROR in scrape_google_search: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        if driver:
            print(f"DEBUG: Closing browser for query...")
            try:
                driver.quit()
                print(f"Browser closed successfully for query")
            except Exception as e:
                print(f"DEBUG: Browser already closed or error occurred while closing: {e}")
        # Remove the driver from thread_drivers if it was registered
        if job_id and thread_drivers is not None and job_id in thread_drivers:
            try:
                del thread_drivers[job_id]
                print(f"Removed driver for job {job_id} from thread_drivers")
            except KeyError:
                print(f"Driver for job {job_id} was already removed from thread_drivers")
            except Exception as e:
                print(f"Error removing driver for job {job_id} from thread_drivers: {e}")